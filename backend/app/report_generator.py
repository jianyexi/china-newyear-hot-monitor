"""
æ¯æ—¥/æ¯å‘¨è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆå™¨
"""

import datetime
import json
import logging

from sqlalchemy import select, func, distinct
from sqlalchemy.ext.asyncio import AsyncSession

from app.models import HotTopic, DailyReport
from app.analyzer import generate_analysis
from app.schemas import HotTopicOut

logger = logging.getLogger(__name__)


async def generate_daily_report(db: AsyncSession, report_date: str | None = None) -> DailyReport | None:
    """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
    if not report_date:
        report_date = datetime.datetime.now(datetime.timezone.utc).strftime("%Y-%m-%d")

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = (await db.execute(
        select(DailyReport).where(DailyReport.report_date == report_date, DailyReport.report_type == "daily")
    )).scalar()
    if existing:
        return existing

    # èŽ·å–å½“å¤©æ•°æ®
    day_start = datetime.datetime.strptime(report_date, "%Y-%m-%d").replace(tzinfo=datetime.timezone.utc)
    day_end = day_start + datetime.timedelta(days=1)

    query = select(HotTopic).where(
        HotTopic.fetched_at >= day_start,
        HotTopic.fetched_at < day_end,
    ).order_by(HotTopic.fetched_at.desc(), HotTopic.rank)

    result = await db.execute(query)
    all_topics = result.scalars().all()

    if not all_topics:
        return None

    # å–æœ€è¿‘ä¸€æ‰¹åšåˆ†æž
    latest_time = max(t.fetched_at for t in all_topics)
    latest_topics = [HotTopicOut.model_validate(t) for t in all_topics if t.fetched_at == latest_time]

    analysis = await generate_analysis(latest_topics)

    platforms = list({t.platform for t in all_topics})
    total_unique = len({t.title for t in all_topics})

    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    lines = [
        f"# ðŸ“Š çƒ­æœæ—¥æŠ¥ â€” {report_date}",
        "",
        f"## æ¦‚è§ˆ",
        f"- ç›‘æŽ§å¹³å°ï¼š{', '.join(platforms)}",
        f"- æ€»æŠ“å–è¯é¢˜æ•°ï¼š{len(all_topics)}",
        f"- åŽ»é‡è¯é¢˜æ•°ï¼š{total_unique}",
        f"- æ˜¥èŠ‚ç›¸å…³å æ¯”ï¼š{analysis.cny_summary.get('ratio', 0) * 100:.1f}%",
        "",
        f"## åˆ†ç±»åˆ†å¸ƒ",
    ]

    for cat in analysis.categories[:8]:
        lines.append(f"- {cat.category}: {cat.count} æ¡ ({cat.percentage}%)")

    lines.append("")
    lines.append("## è·¨å¹³å°çƒ­ç‚¹")
    if analysis.cross_platform_hot:
        for hot in analysis.cross_platform_hot[:5]:
            titles = list(hot.get("titles", {}).values())
            lines.append(f"- ðŸ”¥ {titles[0] if titles else hot.get('keyword', '')}")
            lines.append(f"  å‡ºçŽ°åœ¨ï¼š{', '.join(hot.get('platforms', []))}")
    else:
        lines.append("- å½“æ—¥æ— è·¨å¹³å°å…±åŒçƒ­ç‚¹")

    if analysis.sentiment_summary:
        lines.append("")
        lines.append("## èˆ†è®ºæƒ…ç»ª")
        ss = analysis.sentiment_summary
        lines.append(f"- æ­£é¢: {ss.get('positive', 0)} æ¡ | ä¸­æ€§: {ss.get('neutral', 0)} æ¡ | è´Ÿé¢: {ss.get('negative', 0)} æ¡")

    if analysis.ai_analysis:
        lines.append("")
        lines.append("## AI æ·±åº¦åˆ†æž")
        lines.append(analysis.ai_analysis)

    summary = "\n".join(lines)

    report = DailyReport(
        report_date=report_date,
        report_type="daily",
        total_topics=total_unique,
        platforms_covered=json.dumps(platforms),
        summary=summary,
    )
    db.add(report)
    await db.commit()
    await db.refresh(report)
    logger.info("Generated daily report for %s", report_date)
    return report
