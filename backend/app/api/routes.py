import csv
import datetime
import io
import json
import logging
import re
from collections import Counter

from fastapi import APIRouter, Depends, Query, Body, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy import select, func, distinct, or_, and_
from sqlalchemy.ext.asyncio import AsyncSession

from app.cache import cache_get, cache_set, cache_delete
from app.database import get_db
from app.models import HotTopic, TopicLifecycle, DailyReport, AlertRule
from app.schemas import (
    HotTopicOut, PlatformStats, TrendItem, AnalysisReport,
    SearchResult, TopicLifecycleOut, DailyReportOut,
    AlertRuleCreate, AlertRuleOut, CompareResult,
)
from app.config import get_runtime_config, update_runtime_config

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api", tags=["hot-topics"])


@router.get("/topics", response_model=list[HotTopicOut])
async def get_topics(
    platform: str | None = Query(None, description="å¹³å°è¿‡æ»¤"),
    cny_only: bool = Query(False, description="ä»…æ˜¥èŠ‚ç›¸å…³"),
    limit: int = Query(50, ge=1, le=200),
    db: AsyncSession = Depends(get_db),
):
    """è·å–æœ€æ–°çƒ­æœåˆ—è¡¨"""
    cache_key = f"topics:{platform}:{cny_only}:{limit}"
    cached = cache_get(cache_key)
    if cached is not None:
        return cached

    sub = select(func.max(HotTopic.fetched_at))
    if platform:
        sub = sub.where(HotTopic.platform == platform)
    latest_time = (await db.execute(sub)).scalar()
    if not latest_time:
        return []

    query = (
        select(HotTopic)
        .where(HotTopic.fetched_at == latest_time)
        .order_by(HotTopic.rank)
        .limit(limit)
    )
    if platform:
        query = query.where(HotTopic.platform == platform)
    if cny_only:
        query = query.where(HotTopic.is_cny_related == True)  # noqa: E712

    result = await db.execute(query)
    topics = result.scalars().all()
    cache_set(cache_key, topics, ttl_seconds=300)
    return topics


@router.get("/topics/history", response_model=list[HotTopicOut])
async def get_history(
    platform: str | None = Query(None),
    hours: int = Query(24, ge=1, le=168),
    limit: int = Query(200, ge=1, le=1000),
    db: AsyncSession = Depends(get_db),
):
    """è·å–å†å²çƒ­æœ"""
    since = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=hours)
    query = (
        select(HotTopic)
        .where(HotTopic.fetched_at >= since)
        .order_by(HotTopic.fetched_at.desc(), HotTopic.rank)
        .limit(limit)
    )
    if platform:
        query = query.where(HotTopic.platform == platform)
    result = await db.execute(query)
    return result.scalars().all()


@router.get("/trends", response_model=list[TrendItem])
async def get_trends(
    title: str = Query(..., description="è¯é¢˜æ ‡é¢˜å…³é”®è¯"),
    hours: int = Query(24, ge=1, le=168),
    db: AsyncSession = Depends(get_db),
):
    """è·å–è¯é¢˜è¶‹åŠ¿"""
    since = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=hours)
    query = (
        select(HotTopic)
        .where(HotTopic.title.contains(title), HotTopic.fetched_at >= since)
        .order_by(HotTopic.fetched_at)
    )
    result = await db.execute(query)
    rows = result.scalars().all()

    # æŒ‰ platform+title åˆ†ç»„
    groups: dict[str, TrendItem] = {}
    for row in rows:
        key = f"{row.platform}:{row.title}"
        if key not in groups:
            groups[key] = TrendItem(
                title=row.title, platform=row.platform, hot_values=[], timestamps=[]
            )
        groups[key].hot_values.append(row.hot_value or 0)
        groups[key].timestamps.append(row.fetched_at)

    return list(groups.values())


@router.get("/stats", response_model=list[PlatformStats])
async def get_stats(db: AsyncSession = Depends(get_db)):
    """è·å–å„å¹³å°ç»Ÿè®¡"""
    cached = cache_get("stats")
    if cached is not None:
        return cached

    platforms = (await db.execute(select(distinct(HotTopic.platform)))).scalars().all()
    stats = []
    for p in platforms:
        total = (
            await db.execute(
                select(func.count()).where(HotTopic.platform == p)
            )
        ).scalar()
        cny = (
            await db.execute(
                select(func.count()).where(
                    HotTopic.platform == p, HotTopic.is_cny_related == True  # noqa: E712
                )
            )
        ).scalar()
        latest = (
            await db.execute(
                select(func.max(HotTopic.fetched_at)).where(HotTopic.platform == p)
            )
        ).scalar()
        stats.append(PlatformStats(platform=p, total_topics=total, cny_related=cny, latest_fetch=latest))
    cache_set("stats", stats, ttl_seconds=300)
    return stats


@router.get("/analysis", response_model=AnalysisReport)
async def get_analysis(db: AsyncSession = Depends(get_db)):
    """è·å–è‡ªåŠ¨åˆ†ææŠ¥å‘Šï¼šåˆ†ç±»ç»Ÿè®¡ã€è·¨å¹³å°çƒ­ç‚¹ã€æ˜¥èŠ‚ä¸“é¢˜ã€AIæ·±åº¦åˆ†æ"""
    from app.analyzer import generate_analysis

    # å–æœ€æ–°ä¸€æ‰¹æ•°æ®
    latest_time = (await db.execute(select(func.max(HotTopic.fetched_at)))).scalar()
    if not latest_time:
        from app.schemas import AnalysisReport as AR
        import datetime
        return AR(
            generated_at=datetime.datetime.now(datetime.timezone.utc),
            total_topics=0, platforms_covered=[], categories=[],
            cross_platform_hot=[], platform_insights=[], cny_summary={},
        )

    query = select(HotTopic).where(HotTopic.fetched_at == latest_time).order_by(HotTopic.rank)
    result = await db.execute(query)
    topics = [HotTopicOut.model_validate(t) for t in result.scalars().all()]

    return await generate_analysis(topics)


# ---- é…ç½®ç®¡ç† API ----

@router.get("/config")
async def get_config():
    """è·å–å½“å‰ç³»ç»Ÿé…ç½®"""
    return get_runtime_config()


@router.put("/config")
async def set_config(updates: dict = Body(...)):
    """æ›´æ–°è¿è¡Œæ—¶é…ç½®ï¼ˆä¸é‡å¯ç”Ÿæ•ˆï¼‰"""
    try:
        new_config = update_runtime_config(updates)
    except (ValueError, Exception) as e:
        raise HTTPException(status_code=422, detail=str(e))
    cache_delete()  # é…ç½®å˜æ›´åæ¸…é™¤ç¼“å­˜

    # å¦‚æœæ›´æ–°äº†æŠ“å–é—´éš”ï¼Œé‡æ–°è°ƒåº¦å®šæ—¶ä»»åŠ¡
    if "scrape_interval_minutes" in updates:
        from app.main import scheduler, run_scrapers
        jobs = scheduler.get_jobs()
        for job in jobs:
            scheduler.remove_job(job.id)
        scheduler.add_job(
            run_scrapers, "interval",
            minutes=new_config["scrape_interval_minutes"],
        )

    return {"message": "é…ç½®å·²æ›´æ–°", "config": new_config}


@router.post("/scrape")
async def trigger_scrape():
    """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æŠ“å–"""
    import asyncio
    from app.main import run_scrapers
    asyncio.create_task(run_scrapers())
    return {"message": "æŠ“å–ä»»åŠ¡å·²è§¦å‘ï¼Œè¯·ç¨ååˆ·æ–°æŸ¥çœ‹ç»“æœ"}


@router.get("/config/platforms")
async def get_available_platforms():
    """è·å–æ‰€æœ‰å¯ç”¨å¹³å°åˆ—è¡¨"""
    return {
        "available": [
            {"id": "weibo", "name": "å¾®åš", "icon": "ğŸ“±"},
            {"id": "zhihu", "name": "çŸ¥ä¹", "icon": "ğŸ’¬"},
            {"id": "baidu", "name": "ç™¾åº¦", "icon": "ğŸ”"},
            {"id": "douyin", "name": "æŠ–éŸ³", "icon": "ğŸµ"},
            {"id": "xiaohongshu", "name": "å°çº¢ä¹¦", "icon": "ğŸ“•"},
        ],
        "enabled": get_runtime_config()["enabled_platforms"],
    }


@router.get("/export/csv")
async def export_csv(
    platform: str | None = Query(None, description="å¹³å°è¿‡æ»¤"),
    hours: int = Query(24, ge=1, le=168),
    db: AsyncSession = Depends(get_db),
):
    """å¯¼å‡ºçƒ­æœæ•°æ®ä¸º CSV"""
    since = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=hours)
    query = (
        select(HotTopic)
        .where(HotTopic.fetched_at >= since)
        .order_by(HotTopic.fetched_at.desc(), HotTopic.rank)
    )
    if platform:
        query = query.where(HotTopic.platform == platform)

    result = await db.execute(query)
    rows = result.scalars().all()

    output = io.StringIO()
    # Add BOM for Excel UTF-8 compatibility
    output.write('\ufeff')
    writer = csv.writer(output)
    writer.writerow(["æ’å", "å¹³å°", "æ ‡é¢˜", "çƒ­åº¦", "URL", "æ˜¥èŠ‚ç›¸å…³", "æŠ“å–æ—¶é—´"])
    for r in rows:
        writer.writerow([
            r.rank, r.platform, r.title, r.hot_value or "",
            r.url or "", "æ˜¯" if r.is_cny_related else "å¦",
            r.fetched_at.isoformat() if r.fetched_at else "",
        ])

    output.seek(0)
    filename = f"hot_topics_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv; charset=utf-8",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


# ---- å…¨æ–‡æœç´¢ ----

@router.get("/search", response_model=SearchResult)
async def search_topics(
    keyword: str = Query(..., min_length=1, max_length=100, description="æœç´¢å…³é”®è¯"),
    platform: str | None = Query(None),
    hours: int = Query(24, ge=1, le=720),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
):
    """å…¨æ–‡æœç´¢çƒ­æœè¯é¢˜"""
    since = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=hours)
    conditions = [HotTopic.fetched_at >= since, HotTopic.title.contains(keyword)]
    if platform:
        conditions.append(HotTopic.platform == platform)

    # æ€»æ•°
    total = (await db.execute(select(func.count()).where(*conditions))).scalar() or 0

    # åˆ†é¡µæŸ¥è¯¢
    query = (
        select(HotTopic)
        .where(*conditions)
        .order_by(HotTopic.fetched_at.desc(), HotTopic.rank)
        .offset((page - 1) * page_size)
        .limit(page_size)
    )
    result = await db.execute(query)
    items = [HotTopicOut.model_validate(t) for t in result.scalars().all()]

    return SearchResult(total=total, page=page, page_size=page_size, items=items)


# ---- è¯é¢˜ç”Ÿå‘½å‘¨æœŸ ----

@router.get("/lifecycle", response_model=list[TopicLifecycleOut])
async def get_lifecycle(
    status: str | None = Query(None, description="rising/peak/falling/off"),
    platform: str | None = Query(None),
    limit: int = Query(50, ge=1, le=200),
    db: AsyncSession = Depends(get_db),
):
    """è·å–è¯é¢˜ç”Ÿå‘½å‘¨æœŸæ•°æ®"""
    query = select(TopicLifecycle).order_by(TopicLifecycle.last_seen.desc()).limit(limit)
    if status:
        query = query.where(TopicLifecycle.status == status)
    if platform:
        query = query.where(TopicLifecycle.platform == platform)
    result = await db.execute(query)
    return result.scalars().all()


# ---- æ¯æ—¥æŠ¥å‘Š ----

@router.get("/reports", response_model=list[DailyReportOut])
async def list_reports(
    report_type: str = Query("daily", description="daily/weekly"),
    limit: int = Query(30, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
):
    """è·å–æŠ¥å‘Šåˆ—è¡¨"""
    query = (
        select(DailyReport)
        .where(DailyReport.report_type == report_type)
        .order_by(DailyReport.report_date.desc())
        .limit(limit)
    )
    result = await db.execute(query)
    return result.scalars().all()


@router.get("/reports/{report_date}")
async def get_report(
    report_date: str,
    report_type: str = Query("daily"),
    db: AsyncSession = Depends(get_db),
):
    """è·å–æŒ‡å®šæ—¥æœŸçš„æŠ¥å‘Š"""
    result = await db.execute(
        select(DailyReport).where(
            DailyReport.report_date == report_date,
            DailyReport.report_type == report_type,
        )
    )
    report = result.scalar()
    if not report:
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šä¸å­˜åœ¨")
    return DailyReportOut.model_validate(report)


@router.post("/reports/generate")
async def trigger_report(
    report_date: str = Query(None, description="YYYY-MM-DDï¼Œé»˜è®¤ä»Šå¤©"),
    db: AsyncSession = Depends(get_db),
):
    """æ‰‹åŠ¨ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„æŠ¥å‘Š"""
    from app.report_generator import generate_daily_report
    report = await generate_daily_report(db, report_date)
    if not report:
        raise HTTPException(status_code=404, detail="è¯¥æ—¥æœŸæ— æ•°æ®")
    return {"message": "æŠ¥å‘Šå·²ç”Ÿæˆ", "report_date": report.report_date}


# ---- å‘Šè­¦è§„åˆ™ç®¡ç† ----

@router.get("/alerts", response_model=list[AlertRuleOut])
async def list_alerts(db: AsyncSession = Depends(get_db)):
    """è·å–æ‰€æœ‰å‘Šè­¦è§„åˆ™"""
    result = await db.execute(select(AlertRule).order_by(AlertRule.id))
    return result.scalars().all()


@router.post("/alerts", response_model=AlertRuleOut)
async def create_alert(rule: AlertRuleCreate, db: AsyncSession = Depends(get_db)):
    """åˆ›å»ºå‘Šè­¦è§„åˆ™"""
    alert = AlertRule(**rule.model_dump())
    db.add(alert)
    await db.commit()
    await db.refresh(alert)
    return alert


@router.delete("/alerts/{alert_id}")
async def delete_alert(alert_id: int, db: AsyncSession = Depends(get_db)):
    """åˆ é™¤å‘Šè­¦è§„åˆ™"""
    result = await db.execute(select(AlertRule).where(AlertRule.id == alert_id))
    alert = result.scalar()
    if not alert:
        raise HTTPException(status_code=404, detail="å‘Šè­¦è§„åˆ™ä¸å­˜åœ¨")
    await db.delete(alert)
    await db.commit()
    return {"message": "å·²åˆ é™¤"}


# ---- æ—¶é—´å¯¹æ¯”åˆ†æ ----

@router.get("/compare", response_model=CompareResult)
async def compare_periods(
    hours_ago_1: int = Query(0, ge=0, description="ç¬¬ä¸€ä¸ªæ—¶æ®µï¼šå‡ å°æ—¶å‰"),
    hours_ago_2: int = Query(24, ge=1, description="ç¬¬äºŒä¸ªæ—¶æ®µï¼šå‡ å°æ—¶å‰"),
    platform: str | None = Query(None),
    db: AsyncSession = Depends(get_db),
):
    """å¯¹æ¯”ä¸¤ä¸ªæ—¶é—´æ®µçš„çƒ­æœå˜åŒ–"""
    now = datetime.datetime.now(datetime.timezone.utc)

    async def get_period_topics(hours_ago: int) -> list[HotTopic]:
        target_time = now - datetime.timedelta(hours=hours_ago)
        sub = select(func.max(HotTopic.fetched_at)).where(HotTopic.fetched_at <= target_time)
        if platform:
            sub = sub.where(HotTopic.platform == platform)
        closest_time = (await db.execute(sub)).scalar()
        if not closest_time:
            return []
        query = select(HotTopic).where(HotTopic.fetched_at == closest_time)
        if platform:
            query = query.where(HotTopic.platform == platform)
        result = await db.execute(query)
        return list(result.scalars().all())

    topics1 = await get_period_topics(hours_ago_1)
    topics2 = await get_period_topics(hours_ago_2)

    titles1 = {t.title for t in topics1}
    titles2 = {t.title for t in topics2}
    rank_map1 = {t.title: t.rank for t in topics1}
    rank_map2 = {t.title: t.rank for t in topics2}

    new_topics = sorted(titles1 - titles2)[:20]
    dropped_topics = sorted(titles2 - titles1)[:20]

    common = titles1 & titles2
    rising = []
    falling = []
    for title in common:
        r1, r2 = rank_map1.get(title, 99), rank_map2.get(title, 99)
        if r1 < r2:
            rising.append({"title": title, "rank_before": r2, "rank_after": r1, "change": r2 - r1})
        elif r1 > r2:
            falling.append({"title": title, "rank_before": r2, "rank_after": r1, "change": r1 - r2})

    rising.sort(key=lambda x: x["change"], reverse=True)
    falling.sort(key=lambda x: x["change"], reverse=True)

    return CompareResult(
        period1=f"{hours_ago_1}h ago",
        period2=f"{hours_ago_2}h ago",
        new_topics=new_topics,
        dropped_topics=dropped_topics,
        rising_topics=rising[:10],
        falling_topics=falling[:10],
        common_count=len(common),
    )


# ---- è¯äº‘æ•°æ® ----

@router.get("/wordcloud")
async def get_wordcloud(
    hours: int = Query(24, ge=1, le=168),
    platform: str | None = Query(None),
    db: AsyncSession = Depends(get_db),
):
    """è·å–è¯é¢‘æ•°æ®ï¼ˆç”¨äºè¯äº‘å±•ç¤ºï¼‰"""
    cache_key = f"wordcloud:{platform}:{hours}"
    cached = cache_get(cache_key)
    if cached is not None:
        return cached

    since = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=hours)
    query = select(HotTopic.title).where(HotTopic.fetched_at >= since)
    if platform:
        query = query.where(HotTopic.platform == platform)

    result = await db.execute(query)
    titles = [r[0] for r in result]

    # æå–è¯é¢‘
    word_counter: Counter = Counter()
    for title in titles:
        words = re.findall(r"[\u4e00-\u9fff]{2,4}", title)
        word_counter.update(words)

    # è¿‡æ»¤åœç”¨è¯
    stopwords = {"ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "å¯ä»¥", "å°±æ˜¯", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª", "ä¸æ˜¯"}
    data = [
        {"name": word, "value": count}
        for word, count in word_counter.most_common(200)
        if word not in stopwords
    ]

    cache_set(cache_key, data, ttl_seconds=300)
    return data


# ---- æƒ…æ„Ÿåˆ†æç»Ÿè®¡ ----

@router.get("/sentiment")
async def get_sentiment_stats(
    platform: str | None = Query(None),
    db: AsyncSession = Depends(get_db),
):
    """è·å–æƒ…æ„Ÿåˆ†æç»Ÿè®¡"""
    latest_time = (await db.execute(select(func.max(HotTopic.fetched_at)))).scalar()
    if not latest_time:
        return {"positive": 0, "neutral": 0, "negative": 0, "details": []}

    query = select(HotTopic).where(HotTopic.fetched_at == latest_time)
    if platform:
        query = query.where(HotTopic.platform == platform)

    result = await db.execute(query)
    topics = result.scalars().all()

    sentiment_counts = Counter(t.sentiment or "neutral" for t in topics)
    details = [
        {
            "title": t.title,
            "platform": t.platform,
            "sentiment": t.sentiment or "neutral",
            "score": t.sentiment_score or 0,
            "rank": t.rank,
        }
        for t in sorted(topics, key=lambda x: abs(x.sentiment_score or 0), reverse=True)[:20]
    ]

    return {
        "positive": sentiment_counts.get("positive", 0),
        "neutral": sentiment_counts.get("neutral", 0),
        "negative": sentiment_counts.get("negative", 0),
        "total": len(topics),
        "details": details,
    }
